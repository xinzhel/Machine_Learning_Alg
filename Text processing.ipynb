{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Tokenization: \n",
    "it will split sentence into segments by space or puctunations. Then we can use these tokens as part of machine learning algorithm. How we tokenize text in our DataFrame can affect the statistics we use in our model.\n",
    "* Bag-of-words:  count the number of times a particular token appears. However, it discards information about word order.\n",
    "* N-gram: In addition to a column for every token which is called \"1-gram\", we may have a column for every ordered pair of N words.  \n",
    "See [More](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the vocabulary using `Counter`, which is a dictionary of words and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 155 tokens in data if we split on non-alpha numeric\n",
      "dict_items([('If', 1), ('you', 3), ('like', 4), ('adult', 1), ('comedy', 1), ('cartoons,', 1), ('South', 1), ('Park,', 1), ('then', 1), ('this', 2), ('is', 6), ('nearly', 1), ('a', 4), ('similar', 1), ('format', 1), ('about', 1), ('the', 15), ('small', 2), ('adventures', 1), ('of', 2), ('three', 1), ('teenage', 1), ('girls', 1), ('at', 3), ('Bromwell', 1), ('High.', 1), ('Keisha,', 1), ('Natella', 1), ('and', 6), ('Latrina', 1), ('have', 3), ('given', 1), ('exploding', 1), ('sweets', 1), ('behaved', 1), ('bitches,', 1), ('I', 2), ('think', 1), ('Keisha', 1), ('good', 1), ('leader.', 1), ('There', 1), ('are', 1), ('also', 2), ('stories', 1), ('going', 1), ('on', 1), ('with', 1), ('teachers', 1), ('school.', 1), (\"There's\", 1), ('idiotic', 1), ('principal,', 1), ('Mr.', 1), ('Bip,', 1), ('nervous', 1), ('Maths', 1), ('teacher', 1), ('many', 1), ('others.', 1), ('The', 2), ('cast', 1), ('fantastic,', 1), ('Lenny', 1), (\"Henry's\", 1), ('Gina', 1), ('Yashere,', 1), ('EastEnders', 1), ('Chrissie', 1), ('Watts,', 1), ('Tracy-Ann', 1), ('Oberman,', 1), ('Smack', 1), (\"Pony's\", 1), ('Doon', 1), ('Mackichan,', 1), ('Dead', 1), (\"Ringers'\", 1), ('Mark', 1), ('Perry', 1), (\"Blunder's\", 1), ('Nina', 1), ('Conti.', 1), (\"didn't\", 1), ('know', 1), ('came', 1), ('from', 1), ('Canada,', 1), ('but', 2), ('it', 4), ('very', 1), ('good.', 1), ('Very', 1), ('good!', 1), ('All', 1), (\"world's\", 1), ('stage', 1), ('its', 1), ('people', 1), ('actors', 1), ('in', 2), ('it\"--or', 1), ('something', 1), ('that.', 1), ('Who', 1), ('hell', 1), ('said', 1), ('that', 3), ('theatre', 2), ('stopped', 1), ('orchestra', 1), ('pit--or', 1), ('even', 1), ('door?', 1), ('Why', 1), ('not', 3), ('audience', 1), ('participants', 1), ('theatrical', 1), ('experience,', 1), ('including', 1), ('story', 3), ('itself?<br', 1), ('/><br', 2), ('/>This', 1), ('film', 1), ('was', 1), ('grand', 1), ('experiment', 1), ('said:', 1), ('\"Hey!', 1), ('needs', 2), ('more', 1), ('than', 1), ('your', 2), ('attention,', 1), ('active', 1), ('participation\".', 1), ('\"Sometimes', 1), ('we', 1), ('bring', 1), ('to', 3), ('you,', 1), ('sometimes', 1), ('go', 1), ('story.\"<br', 1), ('/>Alas', 1), ('no', 1), ('one', 1), ('listened,', 1), ('does', 1), ('mean', 1), ('should', 1), ('been', 1), ('said.', 1)])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "for data in x:\n",
    "    tokens = data.split()\n",
    "    vocab.update(tokens)\n",
    "\n",
    "tokens = [k for k,c in vocab.items()]\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in data if we split on non-alpha numeric\"\n",
    "print(msg.format(len(tokens)))\n",
    "print(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit learn` provides `CountVectorizer()` for bag-of-words. It will do:\n",
    "* Tokenize all the strings\n",
    "* Build a 'vocabulary' containing all the tokens\n",
    "* Count the occurences of each token in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 151 tokens in data if we split on non-alpha numeric\n",
      "['\"hey!', '\"sometimes', '/><br', '/>alas', '/>this', 'a', 'about', 'active', 'actors', 'adult', 'adventures', 'all', 'also', 'and', 'are', 'at', 'attention,', 'audience', 'been', 'behaved', 'bip,', 'bitches,', \"blunder's\", 'bring', 'bromwell', 'but', 'came', 'canada,', 'cartoons,', 'cast', 'chrissie', 'comedy', 'conti.', 'dead', \"didn't\", 'does', 'doon', 'door?', 'eastenders', 'even', 'experience,', 'experiment', 'exploding', 'fantastic,', 'film', 'format', 'from', 'gina', 'girls', 'given', 'go', 'going', 'good', 'good.', 'grand', 'have', 'hell', \"henry's\", 'high.', 'i', 'idiotic', 'if', 'in', 'including', 'is', 'it', 'it\"--or', 'its', 'itself?<br', 'keisha', 'keisha,', 'know', 'latrina', 'leader.', 'lenny', 'like', 'listened,', 'mackichan,', 'many', 'mark', 'maths', 'mean', 'more', 'mr.', 'natella', 'nearly', 'needs', 'nervous', 'nina', 'no', 'not', 'oberman,', 'of', 'on', 'one', 'orchestra', 'others.', 'park,', 'participants', 'participation\".', 'people', 'perry', 'pit--or', \"pony's\", 'principal,', \"ringers'\", 'said', 'said:', 'school.', 'should', 'similar', 'smack', 'small', 'something', 'sometimes', 'south', 'stage', 'stopped', 'stories', 'story', 'story.\"<br', 'sweets', 'teacher', 'teachers', 'teenage', 'than', 'that', 'that.', 'the', 'theatre', 'theatrical', 'then', 'there', \"there's\", 'think', 'this', 'three', 'to', 'tracy-ann', 'very', 'was', 'watts,', 'we', 'who', 'why', 'with', \"world's\", 'yashere,', 'you', 'you,', 'your']\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='\\\\S+(?=\\\\s+)',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "data1 = \"If you like adult comedy cartoons, like South Park, then this is nearly a similar format about the small adventures of three teenage girls at Bromwell High. Keisha, Natella and Latrina have given exploding sweets and behaved like bitches, I think Keisha is a good leader. There are also small stories going on with the teachers of the school. There's the idiotic principal, Mr. Bip, the nervous Maths teacher and many others. The cast is also fantastic, Lenny Henry's Gina Yashere, EastEnders Chrissie Watts, Tracy-Ann Oberman, Smack The Pony's Doon Mackichan, Dead Ringers' Mark Perry and Blunder's Nina Conti. I didn't know this came from Canada, but it is very good. Very good!\"\n",
    "data2 = 'All the world\\'s a stage and its people actors in it\"--or something like that. Who the hell said that theatre stopped at the orchestra pit--or even at the theatre door? Why is not the audience participants in the theatrical experience, including the story itself?<br /><br />This film was a grand experiment that said: \"Hey! the story is you and it needs more than your attention, it needs your active participation\". \"Sometimes we bring the story to you, sometimes you have to go to the story.\"<br /><br />Alas no one listened, but that does not mean it should not have been said.'\n",
    "x = pd.Series([data1, data2]) \n",
    "\n",
    "# Define a regular expression that does a split on whitespace\n",
    "TOKEN_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer\n",
    "vec_basic = CountVectorizer(token_pattern=TOKEN_BASIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_basic.fit(x)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in data if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_basic.get_feature_names())))\n",
    "print(vec_basic.get_feature_names())\n",
    "print(vec_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "Tokenize on punctuation to avoid hyphens, underscores, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "By including unigrams and bi-grams in the model to capture important information involving multiple tokens, it is more likely to capture the information that appears as multiple tokens like \"middle school\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the token pattern that contains only alphanumeric characters.\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer\n",
    "vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                      ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `PolynomialFeatures` to tell it the degree of features to include.  \n",
    "**Interaction terms** let us mathematically describe when tokens appear together.  \n",
    "$$ \\beta_{1} x_{1}+\\beta_{2} x_{2}+\\beta_{3}\\left(x_{1} \\times x_{2}\\right) $$  \n",
    "$ \\beta_{3} $ measure how important it is that X1 and X2 appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we looked at multiplying 2 columns together to see if they co-occurred. `include_bias` defines the bias term which allows model to have non-zero y value where x value is zero. The argument `degree` tells it what polynomial degree of interactions to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2\n",
       "a   0   1\n",
       "b   1   1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "data = {\"x1\": [0, 1], \"x2\": [1, 1]}\n",
    "x = pd.DataFrame(data, columns=['x1', 'x2'])\n",
    "x.rename(index={0:'a',1:'b'}, inplace=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction  = PolynomialFeatures(degree=2,\n",
    "                                  interaction_only=True,\n",
    "                                  include_bias=False)\n",
    "\n",
    "interaction.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X array will grow expotentially since we are making n features into n-squared features.  \n",
    "Because `PolynomialFeatures` does not support sparse matrices, we can use a custom interaction object, `SparseInteractions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseInteractions(degree=2).fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we need to balance adding new features with computational cost of additional columns. For example, adding 3-grams or 4-grams is going to have an enormous increase in the size of the array. So we need more computational power to fit out model.  \n",
    "Hashing is a way of limiting the size of matrix that we create without sacrificing too much model accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hash function takes an input, in text case a token, and outputs a hash value. For example, the input may be a string and the hash value may be an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use the hashing trick, we want to make array of features as small as possible. Doing so is called **Dimensionality Reduction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn`, we can use `HashingVectorizer` instead of the `CountVectorizer`. `HashingVectorizer` maps every token to one of those pre-defined number of columns. Some columns may have multiple tokens that map to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vec = HashingVectorizer(norm=None,\n",
    "                        token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                        ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "np.array(x) > 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
