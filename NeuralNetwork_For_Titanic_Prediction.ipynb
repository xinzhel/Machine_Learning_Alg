{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality data to quantity\n",
    "df = df.replace([\"male\", \"female\"], [0,1])\n",
    "df = df.replace([\"S\", \"C\", \"Q\"], [0,1,2])\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"Survived\"]]\n",
    "\n",
    "#predictors = data.drop(['Survived'], axis=1)\n",
    "X = df[[\"PassengerId\",\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]]\n",
    "\n",
    "# convert to numpy array for NN\n",
    "X = X.astype(np.float32).values\n",
    "y = to_categorical(df.Survived)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/30\n",
      "623/623 [==============================] - 0s 487us/sample - loss: 73.8389 - acc: 0.5554 - val_loss: 0.6796 - val_acc: 0.6455\n",
      "Epoch 2/30\n",
      "623/623 [==============================] - 0s 77us/sample - loss: 0.7842 - acc: 0.5939 - val_loss: 0.6687 - val_acc: 0.6493\n",
      "Epoch 3/30\n",
      "623/623 [==============================] - 0s 76us/sample - loss: 0.7200 - acc: 0.5955 - val_loss: 0.6678 - val_acc: 0.6418\n",
      "Epoch 4/30\n",
      "623/623 [==============================] - 0s 73us/sample - loss: 0.7441 - acc: 0.5987 - val_loss: 0.6599 - val_acc: 0.6455\n",
      "Epoch 5/30\n",
      "623/623 [==============================] - 0s 83us/sample - loss: 0.6877 - acc: 0.6035 - val_loss: 0.6591 - val_acc: 0.6455\n",
      "Epoch 6/30\n",
      "623/623 [==============================] - 0s 84us/sample - loss: 0.6762 - acc: 0.6067 - val_loss: 0.6537 - val_acc: 0.6493\n",
      "Epoch 7/30\n",
      "623/623 [==============================] - 0s 83us/sample - loss: 0.6696 - acc: 0.6132 - val_loss: 0.6538 - val_acc: 0.6493\n",
      "Epoch 8/30\n",
      "623/623 [==============================] - 0s 86us/sample - loss: 0.6774 - acc: 0.6164 - val_loss: 0.6564 - val_acc: 0.6418\n",
      "[0.4913724  0.7389374  0.27269608 0.43935025 0.29885024 0.4450482\n",
      " 0.08135019 0.43146572 0.2287099  0.30054954 0.4304427  0.03321481\n",
      " 0.29947922 0.09071387 0.42197412 0.12840632 0.42121288 0.40483806\n",
      " 0.19013476 0.4011223  0.13686407 0.25711378 0.40420854 0.19899845\n",
      " 0.41538277 0.15491036 0.4018583  0.15418814 0.4018583  0.4018583\n",
      " 0.17888899 0.7066837  0.4018583  0.05863216 0.22791588 0.11051708\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4821103  0.41237873\n",
      " 0.4018583  0.46060556 0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4079733  0.4018583  0.17393269 0.4018583\n",
      " 0.07397439 0.4018583  0.4018583  0.4018583  0.4018583  0.40585643\n",
      " 0.4018583  0.34198818 0.27588388 0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.57981426 0.4018583  0.43218467 0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.5335245  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.46549484 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4039358  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.39410818 0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4139977  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.3911229  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.5949101\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.608896\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.40466058\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583  0.4018583  0.4018583  0.4018583\n",
      " 0.4018583  0.4018583  0.4018583 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (X.shape[1],)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(input_shape):\n",
    "    # Set up the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model.add(Dense(32, activation='relu', input_shape=input_shape))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "891/891 [==============================] - 0s 234us/sample - loss: 79.7446\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "891/891 [==============================] - 0s 280us/sample - loss: 38.7575\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "891/891 [==============================] - 0s 275us/sample - loss: 376353.7088\n"
     ]
    }
   ],
   "source": [
    "# Import the SGD optimizer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model((X.shape[1],))\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: optimizer\n",
    "    optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
